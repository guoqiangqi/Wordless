#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ™ºèƒ½æ–‡æ¡£åˆ†æå™¨ MCP æœåŠ¡å™¨

åŸºäº Wordless NLP åº“å®ç°çš„æ–‡æ¡£åˆ†ææœåŠ¡ï¼Œé€šè¿‡ Model Context Protocol (MCP) æä¾›ï¼š
- å¯è¯»æ€§åˆ†æï¼ˆARIã€Lixã€Coleman-Liau Indexï¼‰
- è¯æ±‡å¤šæ ·æ€§åˆ†æï¼ˆTTRã€RTTRã€CTTRã€Herdan's Cã€Yule's Kï¼‰
- ç»“æ„å¤æ‚åº¦åˆ†æï¼ˆå¥é•¿ã€è¯é•¿ç»Ÿè®¡ï¼‰
- æ™ºèƒ½è¯­è¨€æ£€æµ‹ï¼ˆä¸­è‹±æ–‡ï¼Œç®€ç¹ä½“ï¼Œç¾è‹±å¼ï¼‰

æ”¯æŒçš„MCPåŠŸèƒ½ï¼š
- Tools: 2ä¸ªå·¥å…·ï¼ˆanalyze_document, detect_languageï¼‰
- Resources: 2ä¸ªèµ„æºï¼ˆè¯­è¨€åˆ—è¡¨ã€æŒ‡æ ‡è¯´æ˜ï¼‰
- Prompts: 3ä¸ªæç¤ºæ¨¡æ¿ï¼ˆåˆ†æã€å¯¹æ¯”ã€æ”¹è¿›ï¼‰

ä½¿ç”¨æ–¹æ³•:
    stdioæ¨¡å¼:  python mcp_doc_analyzer_server.py
    HTTPæ¨¡å¼:   python mcp_doc_analyzer_server.py --transport streamable-http --port 8000

å‚è€ƒæ–‡æ¡£: https://github.com/modelcontextprotocol/python-sdk
"""

import argparse
import json
import logging
import os
import re
import sys
from collections import Counter
from pathlib import Path
from typing import Optional

import numpy as np

from mcp.server.fastmcp import FastMCP
from mcp.server.streamable_http import TransportSecuritySettings

# æ·»åŠ Wordlessæ¨¡å—åˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).resolve().parent.parent))

# å»¶è¿Ÿå¯¼å…¥Wordlessæ¨¡å—ï¼ˆä»…åœ¨å®é™…ä½¿ç”¨æ—¶å¯¼å…¥ï¼‰
wl_sentence_tokenization = None
wl_word_tokenization = None
wl_settings_global = None

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class BearerTokenMiddleware:
    """ç®€å•çš„Bearer Tokenè®¤è¯ä¸­é—´ä»¶"""
    
    def __init__(self, app, valid_token: str):
        """
        Args:
            app: ASGIåº”ç”¨
            valid_token: æœ‰æ•ˆçš„Bearer token
        """
        self.app = app
        self.valid_token = valid_token
        logger.info("âœ… Bearer Tokenè®¤è¯å·²å¯ç”¨")
    
    async def __call__(self, scope, receive, send):
        """ASGIè°ƒç”¨"""
        if scope["type"] == "http":
            # æ£€æŸ¥Authorizationå¤´
            headers = dict(scope.get("headers", []))
            auth_header = headers.get(b"authorization", b"").decode()
            
            if auth_header.startswith("Bearer "):
                token = auth_header[7:]  # å»æ‰ "Bearer "å‰ç¼€
                if token == self.valid_token:
                    # è®¤è¯æˆåŠŸï¼Œç»§ç»­å¤„ç†è¯·æ±‚
                    await self.app(scope, receive, send)
                    return
            
            # è®¤è¯å¤±è´¥ï¼Œè¿”å›401
            await send({
                "type": "http.response.start",
                "status": 401,
                "headers": [[b"content-type", b"application/json"]],
            })
            await send({
                "type": "http.response.body",
                "body": b'{"error": "Unauthorized", "message": "Invalid or missing Bearer token"}',
            })
            return
        
        # éHTTPè¯·æ±‚ï¼ˆå¦‚WebSocketï¼‰ï¼Œç›´æ¥ä¼ é€’
        await self.app(scope, receive, send)


def _import_wordless_modules():
    """å»¶è¿Ÿå¯¼å…¥Wordlessæ¨¡å—"""
    global wl_sentence_tokenization, wl_word_tokenization, wl_settings_global
    
    if wl_sentence_tokenization is None:
        try:
            from wordless.wl_nlp import (
                wl_sentence_tokenization as _st,
                wl_word_tokenization as _wt,
            )
            from wordless.wl_settings import wl_settings_global as _wsg
            
            wl_sentence_tokenization = _st
            wl_word_tokenization = _wt
            wl_settings_global = _wsg
        except ImportError as e:
            logger.error(f"æ— æ³•å¯¼å…¥Wordlessæ¨¡å—: {e}")
            logger.error("è¯·ç¡®ä¿å·²å®‰è£…æ‰€æœ‰ä¾èµ–ï¼špip install -r requirements.txt")
            raise


class MockMain:
    """æ¨¡æ‹ŸWordlessä¸»çª—å£å¯¹è±¡ï¼Œæä¾›NLPé…ç½®"""
    
    def __init__(self):
        """åˆå§‹åŒ–é…ç½®"""
        _import_wordless_modules()
        self.settings_global = wl_settings_global.init_settings_global()
        self.settings_custom = {
            'sentence_tokenization': {
                'sentence_tokenizer_settings': {
                    'zho_cn': 'spacy_dependency_parser_zho',
                    'zho_tw': 'spacy_dependency_parser_zho',
                    'eng_us': 'spacy_dependency_parser_eng',
                    'eng_gb': 'spacy_dependency_parser_eng',
                    'other': 'spacy_dependency_parser_eng'
                }
            },
            'word_tokenization': {
                'word_tokenizer_settings': {
                    'zho_cn': 'spacy_zho',
                    'zho_tw': 'spacy_zho',
                    'eng_us': 'spacy_eng',
                    'eng_gb': 'spacy_eng',
                    'other': 'spacy_eng'
                }
            },
            'files': {
                'misc_settings': {
                    'read_files_in_chunks_lines': 1000
                }
            },
            'measures': {
                'lexical_density_diversity': {
                    'hdd': {'sample_size': 42},
                    'msttr': {'num_tokens_in_each_seg': 100},
                    'mtld': {'factor_size': 0.72},
                    'mattr': {'window_size': 100}
                }
            }
        }


class LanguageDetector:
    """æ™ºèƒ½è¯­è¨€æ£€æµ‹å™¨ï¼ˆä¼˜åŒ–ç‰ˆï¼‰"""
    
    # æ‰©å±•ç®€ä½“ä¸­æ–‡é«˜é¢‘å­—ï¼ˆå‰50ä¸ªæœ€å¸¸ç”¨å­—ï¼‰
    SIMPLIFIED_CHARS = 'çš„ä¸€æ˜¯äº†æˆ‘ä¸äººåœ¨ä»–æœ‰è¿™ä¸ªä¸Šä»¬æ¥åˆ°æ—¶å¤§åœ°ä¸ºå­ä¸­ä½ è¯´ç”Ÿå›½å¹´ç€å°±é‚£å’Œè¦å¥¹å‡ºä¹Ÿå¾—é‡Œåè‡ªä»¥ä¼šå®¶å¯ä¸‹è€Œè¿‡å¤©å»èƒ½å¯¹å°ä¹ˆå¿ƒå¤šä¹‹èµ·æˆå¥½çœ‹è§åªåæ²¡ç”¨ä¸»é‡‘å¼€æ‰‹çŸ¥é“äº›æ—¥å››æ­£å½“æƒ³è¡Œç†åˆ†èµ°è§å®è¥¿é¢å±±å®æ˜ç‹ç¾æƒ…ç™¾é¢˜æµ·'
    
    # æ‰©å±•ç¹ä½“ä¸­æ–‡ç‰¹å¾å­—ï¼ˆå°æ¹¾å¸¸ç”¨ç¹ä½“å­—ï¼‰
    TRADITIONAL_CHARS = 'å€‹å€‘èªªè™•æ™‚é–“è©±é ­æ±æ¨™é¡Œé–‹é—œä¿‚æ¢å¹¾ç¨®å­¸éé›»éŒ¢è²·è³£å¯¦éš›èªè­˜è®“è®Šå‹•ç”¢ç™¼ç¾é«”æ¥­æœƒå“¡å°ˆæ¥­è³‡è¨Šç¶²çµ¡é€£ç·šæ‡‰è©²å•é¡Œåœ‹éš›'
    
    # æ‰©å±•è‹±å¼æ‹¼å†™è¯æ±‡
    BRITISH_PATTERNS = [
        'colour', 'favour', 'honour', 'labour', 'neighbour', 'rumour', 'savour',
        'centre', 'theatre', 'metre', 'litre', 'fibre',
        'realise', 'organise', 'recognise', 'analyse', 'summarise',
        'defence', 'offence', 'licence', 'practise',
        'travelled', 'cancelled', 'modelling', 'labelled'
    ]
    
    # æ‰©å±•ç¾å¼æ‹¼å†™è¯æ±‡
    AMERICAN_PATTERNS = [
        'color', 'favor', 'honor', 'labor', 'neighbor', 'rumor', 'savor',
        'center', 'theater', 'meter', 'liter', 'fiber',
        'realize', 'organize', 'recognize', 'analyze', 'summarize',
        'defense', 'offense', 'license', 'practice',
        'traveled', 'canceled', 'modeling', 'labeled'
    ]
    
    @staticmethod
    def detect_language(text: str) -> Optional[str]:
        """
        è‡ªåŠ¨æ£€æµ‹æ–‡æœ¬è¯­è¨€ï¼ˆä¼˜åŒ–ç‰ˆV2ï¼šæ›´å‡†ç¡®çš„æŠ€æœ¯æ–‡æ¡£è¯†åˆ«ï¼‰
        
        æ”¹è¿›è¦ç‚¹ï¼š
        1. æ›´å…¨é¢çš„ä»£ç æ¸…ç†ï¼ˆå˜é‡åã€å‘½ä»¤å‚æ•°ã€æ•´è¡Œå‘½ä»¤ç­‰ï¼‰
        2. åªç»Ÿè®¡æœ‰æ„ä¹‰å­—ç¬¦ï¼ˆæ’é™¤æ ‡ç‚¹ã€æ•°å­—ã€ç¬¦å·ï¼‰
        3. ä½¿ç”¨ç›¸å¯¹æ¯”ä¾‹åˆ¤æ–­ï¼ˆä¸­æ–‡vsè‹±æ–‡ï¼‰è€Œéç»å¯¹é˜ˆå€¼
        4. æ‰©å±•ç®€ç¹ä½“ç‰¹å¾å­—åº“ï¼ˆ100+å­—ç¬¦ï¼‰
        5. æ‰©å±•è‹±ç¾å¼è¯æ±‡åº“ï¼ˆ24ä¸ªè¯æ±‡å¯¹ï¼‰
        6. æ™ºèƒ½è¯†åˆ«å‘½ä»¤è¡Œå’Œä»£ç è¡Œå¹¶ç§»é™¤
        
        è¿”å›: 'zho_cn', 'zho_tw', 'eng_us', 'eng_gb' æˆ– None
        """
        if not text or len(text) == 0:
            return None
        
        # === é˜¶æ®µ1: é¢„å¤„ç†æ¸…ç† ===
        text_cleaned = text
        
        # ç§»é™¤Markdownä»£ç å—
        text_cleaned = re.sub(r'```[\s\S]*?```', ' ', text_cleaned)
        text_cleaned = re.sub(r'`[^`]+`', ' ', text_cleaned)
        
        # ç§»é™¤æ•´è¡Œshellå‘½ä»¤ï¼ˆä»¥å¸¸è§å‘½ä»¤å¼€å¤´çš„è¡Œï¼‰
        command_patterns = r'^\s*(?:docker|bash|sh|python|pip|npm|yarn|git|cd|ls|mkdir|rm|cp|mv|cat|echo|export|source|chmod|chown|wget|curl|make|cmake|gcc|g\+\+|apt|yum|brew|sudo|npu-smi|msprof)[\s\w\-\./]*$'
        text_cleaned = re.sub(command_patterns, ' ', text_cleaned, flags=re.MULTILINE)
        
        # ç§»é™¤URLå’Œé‚®ç®±
        text_cleaned = re.sub(r'https?://[^\s\u4e00-\u9fff]+', ' ', text_cleaned)
        text_cleaned = re.sub(r'\S+@\S+\.\S+', ' ', text_cleaned)
        
        # ç§»é™¤æ–‡ä»¶è·¯å¾„å’Œæ‰©å±•å
        text_cleaned = re.sub(r'[/\\][\w\-/\\]+\.[\w]+', ' ', text_cleaned)
        text_cleaned = re.sub(r'\.(?:sh|py|cpp|h|hpp|c|js|ts|json|xml|yaml|yml|md|txt|log|conf|ini|so|lib|dll|exe|run)\b', ' ', text_cleaned)
        
        # ç§»é™¤å‘½ä»¤è¡Œå‚æ•°å’Œç¯å¢ƒå˜é‡
        text_cleaned = re.sub(r'--[\w\-]+=?[\w\-]*', ' ', text_cleaned)
        text_cleaned = re.sub(r'-[\w](?:\s|$)', ' ', text_cleaned)
        text_cleaned = re.sub(r'\$\{?[\w_]+\}?', ' ', text_cleaned)
        text_cleaned = re.sub(r'%[\w_]+%', ' ', text_cleaned)  # Windowsç¯å¢ƒå˜é‡
        
        # ç§»é™¤ç‰ˆæœ¬å·å’ŒIPåœ°å€
        text_cleaned = re.sub(r'\d+\.\d+\.\d+\.\d+', ' ', text_cleaned)
        text_cleaned = re.sub(r'\bv?\d+\.\d+\.\d+[\w\-\.]*', ' ', text_cleaned)
        
        # ç§»é™¤å˜é‡åæ¨¡å¼ï¼ˆsnake_case, camelCase, CONSTANT_CASEç­‰ï¼‰
        text_cleaned = re.sub(r'\b[a-z]+_[a-z_0-9]+\b', ' ', text_cleaned, flags=re.IGNORECASE)
        text_cleaned = re.sub(r'\b[a-z]+[A-Z][a-zA-Z0-9]*\b', ' ', text_cleaned)
        text_cleaned = re.sub(r'\b[A-Z_]{2,}\b', ' ', text_cleaned)  # å¸¸é‡å
        
        # === é˜¶æ®µ2: ç»Ÿè®¡æœ‰æ„ä¹‰å­—ç¬¦ ===
        chinese_chars = re.findall(r'[\u4e00-\u9fff]', text_cleaned)
        english_chars = re.findall(r'[a-zA-Z]', text_cleaned)
        
        chinese_count_cleaned = len(chinese_chars)
        english_count_cleaned = len(english_chars)
        meaningful_cleaned = chinese_count_cleaned + english_count_cleaned
        
        # æ™ºèƒ½å›é€€ï¼šåªæœ‰åœ¨æ¸…ç†åå‡ ä¹æ²¡æœ‰æœ‰æ•ˆå­—ç¬¦æ—¶æ‰å›é€€
        # å…³é”®ä¼˜åŒ–ï¼šå¦‚æœæ¸…ç†åæœ‰ä¸­æ–‡å†…å®¹ï¼ˆå³ä½¿å¾ˆå°‘ï¼‰ï¼Œä¿ç•™æ¸…ç†ç»“æœ
        if meaningful_cleaned < 5 or (meaningful_cleaned < 10 and chinese_count_cleaned == 0):
            # å›é€€æ¡ä»¶ï¼š1) æ€»æ•°<5å­—ç¬¦ï¼Œæˆ– 2) <10å­—ç¬¦ä¸”æ²¡æœ‰ä¸­æ–‡
            text_cleaned = text
            chinese_chars = re.findall(r'[\u4e00-\u9fff]', text_cleaned)
            english_chars = re.findall(r'[a-zA-Z]', text_cleaned)
        
        chinese_count = len(chinese_chars)
        english_count = len(english_chars)
        meaningful_total = chinese_count + english_count
        
        # å¦‚æœæœ‰æ„ä¹‰å­—ç¬¦å¤ªå°‘ï¼Œæ— æ³•åˆ¤æ–­
        if meaningful_total == 0:
            return None
        
        # === é˜¶æ®µ3: è®¡ç®—ç›¸å¯¹æ¯”ä¾‹ ===
        chinese_ratio = chinese_count / meaningful_total
        english_ratio = english_count / meaningful_total
        
        # === é˜¶æ®µ3.5: å‰å¯¼è¯­è¨€æ£€æµ‹ï¼ˆé’ˆå¯¹æŠ€æœ¯æ–‡æ¡£ä¼˜åŒ–ï¼‰ ===
        # æ£€æµ‹æ–‡æœ¬å¼€å¤´çš„è¯­è¨€å€¾å‘ï¼ˆå‰20ä¸ªæœ‰æ„ä¹‰å­—ç¬¦ï¼‰
        text_start = text_cleaned[:100]  # å–å‰100ä¸ªå­—ç¬¦
        start_chinese = len(re.findall(r'[\u4e00-\u9fff]', text_start))
        start_english = len(re.findall(r'[a-zA-Z]', text_start))
        
        # å¦‚æœå¼€å¤´æœ‰æ˜æ˜¾ä¸­æ–‡ï¼ˆ>=3ä¸ªä¸”æ¯”è‹±æ–‡å¤šï¼‰ï¼Œå¢åŠ ä¸­æ–‡å€¾å‘
        has_chinese_leading = (start_chinese >= 3 and start_chinese >= start_english)
        
        # === é˜¶æ®µ4: è¯­è¨€åˆ¤æ–­ï¼ˆå¤šå±‚æ¬¡æ™ºèƒ½ç­–ç•¥ï¼‰ ===
        
        # ç‰¹æ®Šå¤„ç†1ï¼šæçŸ­æ–‡æœ¬ï¼ˆ<10ä¸ªæœ‰æ„ä¹‰å­—ç¬¦ï¼‰
        if meaningful_total < 10:
            # ä¼˜å…ˆæŒ‰ç»å¯¹æ•°é‡åˆ¤æ–­
            if chinese_count >= 2:
                return 'zho_cn'
            elif english_count >= 4:
                return 'eng_us'
            elif chinese_count > 0:
                return 'zho_cn'
            elif english_count > 0:
                return 'eng_us'
            else:
                return None
        
        # ç‰¹æ®Šå¤„ç†2ï¼šçŸ­æ–‡æœ¬ï¼ˆ10-25å­—ç¬¦ï¼‰- ä½¿ç”¨æ›´ä½çš„é˜ˆå€¼
        elif meaningful_total <= 25:
            # å‰å¯¼ä¸­æ–‡åˆ¤æ–­
            if has_chinese_leading and chinese_count >= 3:
                return 'zho_cn'
            # ä¸­æ–‡ä¿¡æ¯å¯†åº¦é«˜ï¼Œå æ¯”>=20%å°±åˆ¤å®šä¸ºä¸­æ–‡
            elif chinese_count > 0 and chinese_ratio >= 0.2:
                return 'zho_cn'
            elif chinese_count > english_count:
                return 'zho_cn'
            elif english_count > 0:
                return 'eng_us'
            else:
                return None
        
        # é•¿æ–‡æœ¬ï¼ˆ>=20å­—ç¬¦ï¼‰ä½¿ç”¨å¤æ‚ç­–ç•¥
        
        # ç­–ç•¥0: å‰å¯¼ä¸­æ–‡+è¶³å¤Ÿä¸­æ–‡æ•°é‡
        if has_chinese_leading and chinese_count >= 5 and chinese_ratio >= 0.15:
            # ç®€ç¹ä½“åŒºåˆ†
            simplified_count = sum(1 for char in chinese_chars if char in LanguageDetector.SIMPLIFIED_CHARS)
            traditional_count = sum(1 for char in chinese_chars if char in LanguageDetector.TRADITIONAL_CHARS)
            
            if traditional_count >= 5 and traditional_count > simplified_count * 1.3:
                return 'zho_tw'
            else:
                return 'zho_cn'
        
        # ç­–ç•¥1: ä¸­æ–‡æ˜æ˜¾å ä¼˜ï¼ˆ>30%ï¼‰æˆ–ä¸­æ–‡ç•¥å ä¼˜ä¸”ç»å¯¹æ•°é‡è¶³å¤Ÿ
        elif chinese_ratio > 0.3 or (chinese_ratio > 0.15 and chinese_count >= 20):
            # ç®€ç¹ä½“åŒºåˆ†
            simplified_count = sum(1 for char in chinese_chars if char in LanguageDetector.SIMPLIFIED_CHARS)
            traditional_count = sum(1 for char in chinese_chars if char in LanguageDetector.TRADITIONAL_CHARS)
            
            # å¦‚æœç¹ä½“ç‰¹å¾æ˜æ˜¾ï¼ˆè‡³å°‘5ä¸ªç¹ä½“å­—ï¼Œä¸”æ˜æ˜¾å¤šäºç®€ä½“ï¼‰
            if traditional_count >= 5 and traditional_count > simplified_count * 1.3:
                return 'zho_tw'
            else:
                return 'zho_cn'
        
        # ç­–ç•¥2: è‹±æ–‡æ˜æ˜¾å ä¼˜ï¼ˆ>60%ï¼‰
        elif english_ratio > 0.6:
            # è‹±ç¾å¼åŒºåˆ†
            text_lower = text_cleaned.lower()
            british_count = sum(1 for word in LanguageDetector.BRITISH_PATTERNS 
                              if re.search(r'\b' + word + r'\b', text_lower))
            american_count = sum(1 for word in LanguageDetector.AMERICAN_PATTERNS 
                               if re.search(r'\b' + word + r'\b', text_lower))
            
            # åªæœ‰æ˜ç¡®æ£€æµ‹åˆ°æ‹¼å†™å·®å¼‚æ—¶æ‰åŒºåˆ†è‹±ç¾å¼
            if british_count > american_count and british_count >= 2:
                return 'eng_gb'
            elif american_count > british_count and american_count >= 2:
                return 'eng_us'
            else:
                return 'eng_us'  # é»˜è®¤ç¾å¼
        
        # ç­–ç•¥3: æ¯”ä¾‹ç›¸è¿‘ï¼ˆ40%-60%ä¹‹é—´ï¼‰ï¼Œçœ‹è°æ›´å¤š
        elif chinese_ratio > english_ratio:
            return 'zho_cn'
        else:
            return 'eng_us'
    
    @staticmethod
    def get_language_name(lang_code: str) -> str:
        """è·å–è¯­è¨€åç§°"""
        names = {
            'zho_cn': 'ç®€ä½“ä¸­æ–‡',
            'zho_tw': 'ç¹ä½“ä¸­æ–‡',
            'eng_us': 'English (US)',
            'eng_gb': 'English (UK)'
        }
        return names.get(lang_code, 'æœªçŸ¥è¯­è¨€')


class DocumentAnalyzer:
    """æ–‡æ¡£åˆ†ææ ¸å¿ƒå¼•æ“"""
    
    def __init__(self):
        self.main = MockMain()
        logger.info("æ–‡æ¡£åˆ†æå™¨åˆå§‹åŒ–å®Œæˆ")
    
    def analyze_text(self, text: str, language: Optional[str] = None) -> dict:
        """
        åˆ†ææ–‡æœ¬
        
        Args:
            text: è¦åˆ†æçš„æ–‡æœ¬
            language: æŒ‡å®šè¯­è¨€ï¼Œå¦‚æœä¸ºNoneåˆ™è‡ªåŠ¨æ£€æµ‹
            
        Returns:
            åˆ†æç»“æœå­—å…¸
        """
        if not text or not text.strip():
            raise ValueError("æ–‡æœ¬ä¸èƒ½ä¸ºç©º")
        
        # è¯­è¨€æ£€æµ‹
        if language is None:
            language = LanguageDetector.detect_language(text)
            if not language:
                raise ValueError("æ— æ³•è¯†åˆ«æ–‡æ¡£è¯­è¨€")
        
        logger.info(f"åˆ†ææ–‡æœ¬ï¼Œè¯­è¨€: {language}, é•¿åº¦: {len(text)}")
        
        # å¥å­åˆ†è¯
        sentences = wl_sentence_tokenization.wl_sentence_tokenize(
            self.main, text, lang=language
        )
        
        # è¯è¯­åˆ†è¯ï¼ˆåŒæ—¶è®°å½•æ¯ä¸ªå¥å­çš„è¯æ•°ï¼‰
        tokens = []
        sentence_lengths = []
        for sentence in sentences:
            sentence_tokens = wl_word_tokenization.wl_word_tokenize_flat(
                self.main, sentence, lang=language
            )
            tokens.extend(sentence_tokens)
            sentence_lengths.append(len(sentence_tokens))
        
        tokens_text = [str(token) for token in tokens]
        
        # è®¡ç®—æŒ‡æ ‡
        return self._calculate_metrics(tokens_text, sentence_lengths, language)
    
    def _calculate_metrics(self, tokens_text: list, sentence_lengths: list, language: str) -> dict:
        """è®¡ç®—æ‰€æœ‰æŒ‡æ ‡"""
        num_sentences = len(sentence_lengths)
        num_words = len(tokens_text)
        num_chars = sum(len(token) for token in tokens_text)
        
        if num_sentences == 0 or num_words == 0:
            raise ValueError("æ–‡æœ¬è¿‡çŸ­ï¼Œæ— æ³•åˆ†æ")
        
        # 1. å¯è¯»æ€§æŒ‡æ ‡
        ari = 4.71 * (num_chars / num_words) + 0.5 * (num_words / num_sentences) - 21.43
        long_words = sum(1 for token in tokens_text if len(token) > 6)
        lix = (num_words / num_sentences) + (long_words * 100 / num_words)
        
        L = (num_chars / num_words) * 100
        S = (num_sentences / num_words) * 100
        cli = 0.0588 * L - 0.296 * S - 15.8
        
        # 2. è¯æ±‡å¤šæ ·æ€§æŒ‡æ ‡
        num_types = len(set(tokens_text))
        ttr = num_types / num_words
        rttr = num_types / np.sqrt(num_words)
        cttr = num_types / np.sqrt(2 * num_words)
        herdan_c = np.log(num_types) / np.log(num_words) if num_words > 1 else 0
        
        # Yule's K
        tokens_freq = Counter(tokens_text)
        freqs_count = Counter(tokens_freq.values())
        s2 = sum(freq ** 2 * count for freq, count in freqs_count.items())
        yule_k = 10000 * (s2 - num_words) / (num_words ** 2) if num_words > 0 else 0
        
        # 3. ç»“æ„å¤æ‚åº¦æŒ‡æ ‡
        word_lengths = [len(token) for token in tokens_text]
        
        # 4. è¯é¢‘ç»Ÿè®¡ï¼ˆè¿‡æ»¤æ ‡ç‚¹ï¼‰
        tokens_freq_filtered = Counter([
            token for token in tokens_text 
            if any(c.isalnum() for c in token)
        ])
        top_words = tokens_freq_filtered.most_common(10)
        
        # æ„å»ºç»“æœ
        results = {
            'language': {
                'detected': language,
                'name': LanguageDetector.get_language_name(language)
            },
            'readability': {
                'ARI': round(ari, 2),
                'Lix': round(lix, 2),
                'Coleman_Liau_Index': round(cli, 2),
                'interpretation': self._interpret_readability(ari, lix)
            },
            'lexical_diversity': {
                'num_tokens': num_words,
                'num_types': num_types,
                'TTR': round(ttr, 4),
                'RTTR': round(rttr, 4),
                'CTTR': round(cttr, 4),
                'Herdan_C': round(herdan_c, 4),
                'Yule_K': round(yule_k, 2),
                'interpretation': self._interpret_diversity(ttr)
            },
            'structural_complexity': {
                'num_sentences': num_sentences,
                'num_words': num_words,
                'num_chars': num_chars,
                'avg_sentence_length': round(np.mean(sentence_lengths), 2),
                'std_sentence_length': round(np.std(sentence_lengths), 2),
                'max_sentence_length': int(np.max(sentence_lengths)),
                'min_sentence_length': int(np.min(sentence_lengths)),
                'avg_word_length': round(np.mean(word_lengths), 2),
                'std_word_length': round(np.std(word_lengths), 2),
                'interpretation': self._interpret_complexity(
                    np.mean(sentence_lengths), language
                )
            },
            'top_words': [(word, count) for word, count in top_words]
        }
        
        return results
    
    def _interpret_readability(self, ari: float, lix: float) -> str:
        """è§£é‡Šå¯è¯»æ€§"""
        if ari < 10 and lix < 40:
            return "æ˜“è¯» - é€‚åˆå¤§ä¼—è¯»è€…"
        elif ari < 14 and lix < 50:
            return "ä¸­ç­‰ - é€‚åˆé«˜ä¸­åŠä»¥ä¸Šè¯»è€…"
        else:
            return "å›°éš¾ - é€‚åˆä¸“ä¸šè¯»è€…"
    
    def _interpret_diversity(self, ttr: float) -> str:
        """è§£é‡Šè¯æ±‡å¤šæ ·æ€§"""
        if ttr > 0.6:
            return "ä¸°å¯Œ - è¯æ±‡ä½¿ç”¨å¤šæ ·"
        elif ttr > 0.5:
            return "ä¸­ç­‰ - è¯æ±‡ä½¿ç”¨é€‚ä¸­"
        else:
            return "é‡å¤ - è¯æ±‡é‡å¤è¾ƒå¤š"
    
    def _interpret_complexity(self, avg_sent_len: float, language: str) -> str:
        """è§£é‡Šç»“æ„å¤æ‚åº¦"""
        if 'zho' in language:
            if avg_sent_len < 20:
                return "ç®€å• - å¥å­ç»“æ„æ¸…æ™°"
            elif avg_sent_len < 30:
                return "ä¸­ç­‰ - å¥å­ç»“æ„é€‚ä¸­"
            else:
                return "å¤æ‚ - å¥å­è¾ƒé•¿"
        else:
            if avg_sent_len < 15:
                return "ç®€å• - å¥å­ç»“æ„æ¸…æ™°"
            elif avg_sent_len < 20:
                return "ä¸­ç­‰ - å¥å­ç»“æ„é€‚ä¸­"
            else:
                return "å¤æ‚ - å¥å­è¾ƒé•¿"
    
    def format_results_as_text(self, results: dict) -> str:
        """å°†ç»“æœæ ¼å¼åŒ–ä¸ºäººç±»å¯è¯»çš„æ–‡æœ¬"""
        lines = [
            "=" * 60,
            "  æ™ºèƒ½æ–‡æ¡£åˆ†ææŠ¥å‘Š",
            "=" * 60,
            ""
        ]
        
        # è¯­è¨€ä¿¡æ¯
        lang_info = results['language']
        struct_info = results['structural_complexity']
        lines.extend([
            f"ğŸ“Œ æ£€æµ‹è¯­è¨€: {lang_info['name']}",
            f"   æ–‡æ¡£è§„æ¨¡: {struct_info['num_words']} è¯, {struct_info['num_sentences']} å¥",
            ""
        ])
        
        # å¯è¯»æ€§
        read_info = results['readability']
        lines.extend([
            "ğŸ“– å¯è¯»æ€§åˆ†æ:",
            f"   ARIæŒ‡æ•°: {read_info['ARI']}",
            f"   LixæŒ‡æ•°: {read_info['Lix']}",
            f"   Coleman-Liau: {read_info['Coleman_Liau_Index']}",
            f"   ğŸ’¡ {read_info['interpretation']}",
            ""
        ])
        
        # è¯æ±‡å¤šæ ·æ€§
        lex_info = results['lexical_diversity']
        lines.extend([
            "ğŸ“š è¯æ±‡å¤šæ ·æ€§:",
            f"   è¯å‹æ•°/è¯ç¬¦æ•°: {lex_info['num_types']}/{lex_info['num_tokens']}",
            f"   TTR: {lex_info['TTR']}",
            f"   RTTR: {lex_info['RTTR']}",
            f"   Herdan's C: {lex_info['Herdan_C']}",
            f"   ğŸ’¡ {lex_info['interpretation']}",
            ""
        ])
        
        # ç»“æ„å¤æ‚åº¦
        lines.extend([
            "ğŸ” ç»“æ„å¤æ‚åº¦:",
            f"   å¹³å‡å¥é•¿: {struct_info['avg_sentence_length']} è¯",
            f"   å¥é•¿æ ‡å‡†å·®: {struct_info['std_sentence_length']}",
            f"   å¹³å‡è¯é•¿: {struct_info['avg_word_length']} å­—ç¬¦",
            f"   ğŸ’¡ {struct_info['interpretation']}",
            ""
        ])
        
        # é«˜é¢‘è¯
        lines.append("ğŸ“ˆ é«˜é¢‘è¯ Top 10:")
        for i, (word, count) in enumerate(results['top_words'], 1):
            lines.append(f"   {i:2d}. {word:<15} ({count} æ¬¡)")
        
        lines.extend(["", "=" * 60])
        
        return "\n".join(lines)


# MCPæœåŠ¡å™¨å®ä¾‹ï¼ˆåœ¨mainä¸­åˆå§‹åŒ–ï¼‰
mcp: Optional[FastMCP] = None

# å…¨å±€åˆ†æå™¨å®ä¾‹ï¼ˆå»¶è¿Ÿåˆå§‹åŒ–ï¼‰
_analyzer: Optional[DocumentAnalyzer] = None


def get_analyzer() -> DocumentAnalyzer:
    """è·å–åˆ†æå™¨å®ä¾‹ï¼ˆå»¶è¿Ÿåˆå§‹åŒ–ï¼‰"""
    global _analyzer
    if _analyzer is None:
        _analyzer = DocumentAnalyzer()
    return _analyzer


def to_json(data: dict) -> str:
    """ç»Ÿä¸€çš„JSONåºåˆ—åŒ–å‡½æ•°"""
    return json.dumps(data, ensure_ascii=False, indent=2)


def validate_text(text: str) -> None:
    """éªŒè¯æ–‡æœ¬è¾“å…¥"""
    if not text or not text.strip():
        raise ValueError("textå‚æ•°ä¸èƒ½ä¸ºç©º")


def create_mcp_server() -> FastMCP:
    """
    åˆ›å»ºå¹¶é…ç½®MCPæœåŠ¡å™¨
    
    Returns:
        é…ç½®å¥½çš„FastMCPæœåŠ¡å™¨å®ä¾‹
    """
    # é…ç½®ä¼ è¾“å®‰å…¨
    transport_security = TransportSecuritySettings(
        enable_dns_rebinding_protection=False,
        allowed_hosts=["*"],
        allowed_origins=["*"]
    )
    
    # åˆ›å»ºæœåŠ¡å™¨
    server = FastMCP(
        "wordless-doc-analyzer",
        transport_security=transport_security
    )
    
    # æ³¨å†Œå·¥å…·
    @server.tool()
    def analyze_document(
        text: str,
        language: Optional[str] = None,
        format: str = "json"
    ) -> str:
        """
        åˆ†ææ–‡æ¡£æ–‡æœ¬ï¼Œæä¾›å¯è¯»æ€§ã€è¯æ±‡å¤šæ ·æ€§å’Œç»“æ„å¤æ‚åº¦åˆ†æã€‚
        è‡ªåŠ¨æ£€æµ‹ä¸­è‹±æ–‡ï¼Œæ”¯æŒç®€ä½“ä¸­æ–‡ã€ç¹ä½“ä¸­æ–‡ã€è‹±å¼è‹±è¯­å’Œç¾å¼è‹±è¯­ã€‚
        
        Args:
            text: è¦åˆ†æçš„æ–‡æ¡£æ–‡æœ¬å†…å®¹
            language: æŒ‡å®šæ–‡æ¡£è¯­è¨€ï¼ˆå¯é€‰ï¼‰ï¼Œæ”¯æŒ: zho_cn, zho_tw, eng_us, eng_gb
            format: è¾“å‡ºæ ¼å¼ï¼Œjsonï¼ˆç»“æ„åŒ–æ•°æ®ï¼‰æˆ– textï¼ˆäººç±»å¯è¯»ï¼‰ï¼Œé»˜è®¤json
        """
        validate_text(text)
        
        analyzer = get_analyzer()
        results = analyzer.analyze_text(text, language)
        
        return analyzer.format_results_as_text(results) if format == "text" else to_json(results)
    
    @server.tool()
    def detect_language(text: str) -> str:
        """
        æ£€æµ‹æ–‡æœ¬çš„è¯­è¨€ç±»å‹ï¼ˆä¸­æ–‡/è‹±æ–‡ï¼Œç®€ä½“/ç¹ä½“ï¼Œç¾å¼/è‹±å¼ï¼‰
        
        Args:
            text: è¦æ£€æµ‹çš„æ–‡æœ¬
        """
        validate_text(text)
        
        lang = LanguageDetector.detect_language(text)
        if not lang:
            return to_json({"error": "æ— æ³•è¯†åˆ«è¯­è¨€"})
        
        return to_json({
            "language_code": lang,
            "language_name": LanguageDetector.get_language_name(lang)
        })
    
    # æ³¨å†Œèµ„æº
    @server.resource("doc://supported-languages")
    def get_supported_languages() -> str:
        """è·å–æ”¯æŒçš„è¯­è¨€åˆ—è¡¨"""
        return to_json({
            "supported_languages": [
                {"code": "zho_cn", "name": "ç®€ä½“ä¸­æ–‡", "description": "Simplified Chinese"},
                {"code": "zho_tw", "name": "ç¹ä½“ä¸­æ–‡", "description": "Traditional Chinese"},
                {"code": "eng_us", "name": "English (US)", "description": "American English"},
                {"code": "eng_gb", "name": "English (UK)", "description": "British English"}
            ],
            "auto_detection": True,
            "metrics": {
                "readability": ["ARI", "Lix", "Coleman-Liau Index"],
                "lexical_diversity": ["TTR", "RTTR", "CTTR", "Herdan's C", "Yule's K"],
                "structural_complexity": ["å¥é•¿ç»Ÿè®¡", "è¯é•¿ç»Ÿè®¡"]
            }
        })
    
    @server.resource("doc://analysis-metrics")
    def get_analysis_metrics() -> str:
        """è·å–æ‰€æœ‰æ”¯æŒçš„åˆ†ææŒ‡æ ‡è¯´æ˜"""
        return to_json({
            "readability_metrics": {
                "ARI": {
                    "name": "Automated Readability Index",
                    "description": "åŸºäºå­—ç¬¦æ•°å’Œè¯æ•°çš„å¯è¯»æ€§æŒ‡æ•°",
                    "interpretation": "< 10: æ˜“è¯», 10-14: ä¸­ç­‰, > 14: å›°éš¾"
                },
                "Lix": {
                    "name": "LÃ¤sbarhetsindex",
                    "description": "ç‘å…¸å¯è¯»æ€§æŒ‡æ•°ï¼Œè€ƒè™‘é•¿è¯æ¯”ä¾‹",
                    "interpretation": "< 40: æ˜“è¯», 40-50: ä¸­ç­‰, > 50: å›°éš¾"
                },
                "Coleman_Liau_Index": {
                    "name": "Coleman-Liau Index",
                    "description": "åŸºäºå­—ç¬¦å’Œå¥å­çš„å¯è¯»æ€§æŒ‡æ•°"
                }
            },
            "diversity_metrics": {
                "TTR": {
                    "name": "Type-Token Ratio",
                    "description": "è¯å‹ä¸è¯ç¬¦çš„æ¯”ç‡ï¼Œè¡¡é‡è¯æ±‡ä¸°å¯Œåº¦",
                    "interpretation": "> 0.6: ä¸°å¯Œ, 0.5-0.6: ä¸­ç­‰, < 0.5: é‡å¤"
                },
                "RTTR": {
                    "name": "Root Type-Token Ratio",
                    "description": "è¯å‹æ•°é™¤ä»¥è¯ç¬¦æ•°çš„å¹³æ–¹æ ¹"
                },
                "CTTR": {
                    "name": "Corrected Type-Token Ratio",
                    "description": "ä¿®æ­£çš„TTRï¼Œæ›´ç¨³å®š"
                },
                "Herdan_C": {
                    "name": "Herdan's C",
                    "description": "å¯¹æ•°å½¢å¼çš„è¯æ±‡ä¸°å¯Œåº¦æŒ‡æ ‡"
                },
                "Yule_K": {
                    "name": "Yule's K",
                    "description": "åŸºäºè¯é¢‘åˆ†å¸ƒçš„è¯æ±‡å¤šæ ·æ€§æŒ‡æ ‡"
                }
            },
            "structural_metrics": {
                "avg_sentence_length": "å¹³å‡å¥å­é•¿åº¦ï¼ˆè¯æ•°ï¼‰",
                "avg_word_length": "å¹³å‡è¯é•¿ï¼ˆå­—ç¬¦æ•°ï¼‰",
                "sentence_length_std": "å¥é•¿æ ‡å‡†å·®"
            }
        })
    
    # æ³¨å†Œæç¤ºæ¨¡æ¿
    @server.prompt()
    def analyze_document_prompt(text_sample: str) -> str:
        """
        ç”Ÿæˆæ–‡æ¡£åˆ†ææç¤ºè¯
        
        Args:
            text_sample: æ–‡æ¡£æ ·æœ¬æ–‡æœ¬ï¼ˆå¯ä»¥æ˜¯å®Œæ•´æ–‡æ¡£æˆ–æ‘˜è¦ï¼‰
        """
        return f"""è¯·ä½¿ç”¨ analyze_document å·¥å…·åˆ†æä»¥ä¸‹æ–‡æœ¬ï¼š

æ–‡æœ¬å†…å®¹ï¼š
{text_sample}

åˆ†æè¦æ±‚ï¼š
1. è‡ªåŠ¨æ£€æµ‹è¯­è¨€ï¼ˆæ”¯æŒä¸­è‹±æ–‡ï¼‰
2. è¯„ä¼°å¯è¯»æ€§æ°´å¹³ï¼ˆARIã€Lixç­‰æŒ‡æ ‡ï¼‰
3. åˆ†æè¯æ±‡å¤šæ ·æ€§ï¼ˆTTRã€RTTRç­‰ï¼‰
4. è¯„ä¼°ç»“æ„å¤æ‚åº¦ï¼ˆå¥é•¿ã€è¯é•¿ç­‰ï¼‰
5. æä¾›äººç±»å¯è¯»çš„è§£é‡Š

è¯·è°ƒç”¨å·¥å…·å¹¶è§£è¯»ç»“æœã€‚"""
    
    @server.prompt()
    def compare_documents_prompt(text1: str, text2: str, aspect: str = "overall") -> str:
        """
        ç”Ÿæˆæ–‡æ¡£å¯¹æ¯”åˆ†ææç¤ºè¯
        
        Args:
            text1: ç¬¬ä¸€ç¯‡æ–‡æ¡£
            text2: ç¬¬äºŒç¯‡æ–‡æ¡£
            aspect: å¯¹æ¯”ç»´åº¦ï¼ˆoverall/readability/diversity/structureï¼‰
        """
        aspects_desc = {
            "overall": "æ‰€æœ‰æŒ‡æ ‡",
            "readability": "å¯è¯»æ€§",
            "diversity": "è¯æ±‡å¤šæ ·æ€§",
            "structure": "ç»“æ„å¤æ‚åº¦"
        }
        
        return f"""è¯·å¯¹æ¯”åˆ†æä»¥ä¸‹ä¸¤ç¯‡æ–‡æ¡£çš„{aspects_desc.get(aspect, 'æ‰€æœ‰æŒ‡æ ‡')}ï¼š

ã€æ–‡æ¡£1ã€‘
{text1[:500]}...

ã€æ–‡æ¡£2ã€‘
{text2[:500]}...

åˆ†ææ­¥éª¤ï¼š
1. åˆ†åˆ«ä½¿ç”¨ analyze_document å·¥å…·åˆ†æä¸¤ç¯‡æ–‡æ¡£
2. å¯¹æ¯”å…³é”®æŒ‡æ ‡å·®å¼‚
3. è§£é‡Šå·®å¼‚çš„å®é™…æ„ä¹‰
4. ç»™å‡ºæ”¹è¿›å»ºè®®ï¼ˆå¦‚æœé€‚ç”¨ï¼‰

è¯·å¼€å§‹åˆ†æã€‚"""
    
    @server.prompt()
    def readability_improvement_prompt(text: str) -> str:
        """
        ç”Ÿæˆå¯è¯»æ€§æ”¹è¿›å»ºè®®æç¤ºè¯
        
        Args:
            text: éœ€è¦æ”¹è¿›çš„æ–‡æ¡£æ–‡æœ¬
        """
        return f"""è¯·åˆ†æä»¥ä¸‹æ–‡æ¡£çš„å¯è¯»æ€§ï¼Œå¹¶æä¾›æ”¹è¿›å»ºè®®ï¼š

æ–‡æ¡£å†…å®¹ï¼š
{text}

åˆ†ææµç¨‹ï¼š
1. ä½¿ç”¨ analyze_document å·¥å…·è·å–è¯¦ç»†æŒ‡æ ‡
2. è¯†åˆ«å¯è¯»æ€§é—®é¢˜ï¼ˆå¥å­è¿‡é•¿ã€è¯æ±‡è¿‡äºå¤æ‚ç­‰ï¼‰
3. æä¾›å…·ä½“æ”¹è¿›å»ºè®®ï¼š
   - å¥å­é•¿åº¦ä¼˜åŒ–
   - è¯æ±‡é€‰æ‹©å»ºè®®
   - ç»“æ„è°ƒæ•´æ–¹æ¡ˆ
4. å¦‚æœå¯èƒ½ï¼Œç»™å‡ºæ”¹å†™ç¤ºä¾‹

è¯·å¼€å§‹åˆ†æå¹¶æä¾›å»ºè®®ã€‚"""
    
    return server




def configure_http_server(host: str = "0.0.0.0", port: int = 8000):
    """
    é…ç½®FastMCPçš„HTTPæœåŠ¡å™¨å‚æ•°
    
    Args:
        host: æœåŠ¡å™¨åœ°å€ï¼ˆé»˜è®¤: 0.0.0.0ï¼Œç›‘å¬æ‰€æœ‰ç½‘ç»œæ¥å£ï¼‰
        port: æœåŠ¡å™¨ç«¯å£ï¼ˆé»˜è®¤: 8000ï¼‰
    """
    mcp.settings.host = host
    mcp.settings.port = port
    logger.info(f"ğŸš€ HTTPæœåŠ¡å™¨é…ç½®:")
    logger.info(f"   ç›‘å¬åœ°å€: {host}")
    logger.info(f"   ç›‘å¬ç«¯å£: {port}")
    logger.info(f"   è®¿é—®ç«¯ç‚¹: http://{host}:{port}{mcp.settings.streamable_http_path}")
    
    # å®‰å…¨æç¤º
    if host == "0.0.0.0":
        logger.warning(f"âš ï¸  æœåŠ¡å™¨ç›‘å¬æ‰€æœ‰ç½‘ç»œæ¥å£ï¼Œå¯ä»ä»»ä½•IPè®¿é—®")
        logger.warning(f"âš ï¸  ç”Ÿäº§ç¯å¢ƒå»ºè®®é…ç½®é˜²ç«å¢™å’Œè®¿é—®æ§åˆ¶")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description='æ™ºèƒ½æ–‡æ¡£åˆ†æå™¨ MCP æœåŠ¡å™¨ - æä¾›æ–‡æ¡£å¯è¯»æ€§ã€è¯æ±‡å¤šæ ·æ€§å’Œç»“æ„å¤æ‚åº¦åˆ†æ',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
åŠŸèƒ½è¯´æ˜:
  Tools (å·¥å…·):
    â€¢ analyze_document  - å®Œæ•´çš„æ–‡æ¡£åˆ†æï¼ˆå¯è¯»æ€§ã€è¯æ±‡å¤šæ ·æ€§ã€ç»“æ„å¤æ‚åº¦ï¼‰
    â€¢ detect_language   - æ™ºèƒ½è¯­è¨€æ£€æµ‹ï¼ˆæ”¯æŒä¸­è‹±æ–‡ï¼‰
  
  Resources (èµ„æº):
    â€¢ doc://supported-languages - æ”¯æŒçš„è¯­è¨€åˆ—è¡¨
    â€¢ doc://analysis-metrics    - åˆ†ææŒ‡æ ‡è¯¦ç»†è¯´æ˜
  
  Prompts (æç¤ºæ¨¡æ¿):
    â€¢ analyze_document_prompt           - æ–‡æ¡£åˆ†ææç¤º
    â€¢ compare_documents_prompt          - æ–‡æ¡£å¯¹æ¯”åˆ†ææç¤º
    â€¢ readability_improvement_prompt    - å¯è¯»æ€§æ”¹è¿›å»ºè®®æç¤º

è®¤è¯è¯´æ˜:
  ä½¿ç”¨ --auth-token æˆ–ç¯å¢ƒå˜é‡ MCP_AUTH_TOKEN å¯ç”¨Bearer Tokenè®¤è¯
  å®¢æˆ·ç«¯éœ€è¦åœ¨è¯·æ±‚å¤´ä¸­æ·»åŠ : Authorization: Bearer <token>

ç¤ºä¾‹:
  stdioæ¨¡å¼:     python mcp_doc_analyzer_server.py
  HTTPæ¨¡å¼:      python mcp_doc_analyzer_server.py --transport http --host 0.0.0.0 --port 8000
  å¯ç”¨è®¤è¯:      python mcp_doc_analyzer_server.py --transport http --auth-token your-secret-token
  ç¯å¢ƒå˜é‡è®¤è¯:  MCP_AUTH_TOKEN=your-token python mcp_doc_analyzer_server.py --transport http
        """
    )
    parser.add_argument(
        '--transport',
        choices=['stdio', 'http', 'streamable-http'],
        default='stdio',
        help='ä¼ è¾“æ¨¡å¼ï¼ˆé»˜è®¤: stdioï¼‰'
    )
    parser.add_argument(
        '--host',
        type=str,
        default='0.0.0.0',
        help='HTTPæœåŠ¡å™¨åœ°å€ï¼ˆé»˜è®¤: 0.0.0.0ï¼Œç›‘å¬æ‰€æœ‰æ¥å£ï¼‰'
    )
    parser.add_argument(
        '--port',
        type=int,
        default=8000,
        help='HTTPæœåŠ¡å™¨ç«¯å£ï¼ˆé»˜è®¤: 8000ï¼‰'
    )
    parser.add_argument(
        '--auth-token',
        type=str,
        default=None,
        help='è®¤è¯tokenï¼ˆBearer Tokenï¼‰ã€‚ä¹Ÿå¯é€šè¿‡ç¯å¢ƒå˜é‡MCP_AUTH_TOKENè®¾ç½®'
    )
    
    args = parser.parse_args()
    
    try:
        # åˆ›å»ºMCPæœåŠ¡å™¨
        global mcp
        mcp = create_mcp_server()
        
        # è·å–è®¤è¯token
        auth_token = args.auth_token or os.getenv('MCP_AUTH_TOKEN')
        
        if args.transport in ['http', 'streamable-http']:
            logger.info("ğŸŒ å‡†å¤‡å¯åŠ¨è¿œç¨‹HTTPæœåŠ¡...")
            configure_http_server(args.host, args.port)
            
            # å¦‚æœè®¾ç½®äº†è®¤è¯tokenï¼ŒåŒ…è£…åº”ç”¨
            if auth_token:
                # è·å–FastMCPçš„ASGIåº”ç”¨
                app = mcp.streamable_http_app()
                # ä½¿ç”¨è®¤è¯ä¸­é—´ä»¶åŒ…è£…
                wrapped_app = BearerTokenMiddleware(app, auth_token)
                # æ‰‹åŠ¨å¯åŠ¨uvicorn
                import uvicorn
                config = uvicorn.Config(
                    wrapped_app,
                    host=args.host,
                    port=args.port,
                    log_level="info"
                )
                server_instance = uvicorn.Server(config)
                import anyio
                anyio.run(server_instance.serve)
            else:
                logger.warning("âš ï¸  æœªè®¾ç½®è®¤è¯tokenï¼ŒæœåŠ¡å™¨æ— éœ€è®¤è¯")
                mcp.run(transport='streamable-http')
        else:
            logger.info("ğŸš€ å¯åŠ¨ stdio æ¨¡å¼")
            if auth_token:
                logger.warning("âš ï¸  stdioæ¨¡å¼ä¸æ”¯æŒè®¤è¯ï¼Œå¿½ç•¥--auth-tokenå‚æ•°")
            mcp.run()
    except KeyboardInterrupt:
        logger.info("\nâœ… æœåŠ¡å™¨å·²åœæ­¢")
    except Exception as e:
        logger.error(f"âŒ æœåŠ¡å™¨é”™è¯¯: {e}", exc_info=True)
        sys.exit(1)


if __name__ == '__main__':
    main()

