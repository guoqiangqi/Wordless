#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ™ºèƒ½æ–‡æ¡£åˆ†æå™¨ - åŸºäºWordlessæ ¸å¿ƒæ¨¡å—
è‡ªåŠ¨è¯†åˆ«ä¸­è‹±æ–‡ï¼Œæä¾›å¯è¯»æ€§ã€å¯ç”¨æ€§ã€å‘ç°æ€§åˆ†æ

ä½¿ç”¨æ–¹æ³•:
    python analyze_doc.py document.txt
    python analyze_doc.py document.txt --output results.json
"""

import argparse
import sys
import os
import json
import re
from pathlib import Path
from collections import Counter

# æ·»åŠ Wordlessæ¨¡å—åˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).resolve().parent.parent))

import numpy as np

try:
    # æ£€æŸ¥spaCy
    import spacy
except ImportError:
    print(f"âŒ é”™è¯¯: æœªå®‰è£…spaCy")
    print(f"   è¯·è¿è¡Œ: pip install spacy")
    sys.exit(1)

try:
    from wordless.wl_nlp import (
        wl_sentence_tokenization,
        wl_word_tokenization,
        wl_texts
    )
    from wordless.wl_settings import wl_settings_global
    from wordless.wl_utils import wl_misc
except ImportError as e:
    print(f"âŒ é”™è¯¯: æ— æ³•å¯¼å…¥Wordlessæ¨¡å—")
    print(f"   è¯·ç¡®ä¿:")
    print(f"   1. åœ¨Wordlessæ ¹ç›®å½•ä¸‹è¿è¡Œæ­¤è„šæœ¬")
    print(f"   2. å·²å®‰è£…Wordlessçš„æ‰€æœ‰ä¾èµ–")
    print(f"   3. å¦‚æœæ˜¯å¼€å‘ç‰ˆï¼Œè¯·ç¡®ä¿wordlessç›®å½•ç»“æ„å®Œæ•´")
    print(f"\n   è¯¦ç»†é”™è¯¯: {e}")
    print(f"\n   æç¤º: è¿™æ˜¯Wordlessçš„æ‰©å±•å·¥å…·ï¼Œéœ€è¦Wordlessç¯å¢ƒ")
    sys.exit(1)


class MockMain:
    """æ¨¡æ‹ŸWordlessä¸»çª—å£å¯¹è±¡"""
    def __init__(self):
        self.settings_global = wl_settings_global.init_settings_global()
        self.settings_custom = {
            'sentence_tokenization': {
                'sentence_tokenizer_settings': {
                    'zho_cn': 'spacy_dependency_parser_zho',
                    'zho_tw': 'spacy_dependency_parser_zho',
                    'eng_us': 'spacy_dependency_parser_eng',
                    'eng_gb': 'spacy_dependency_parser_eng',
                    'other': 'spacy_dependency_parser_eng'
                }
            },
            'word_tokenization': {
                'word_tokenizer_settings': {
                    'zho_cn': 'spacy_zho',
                    'zho_tw': 'spacy_zho',
                    'eng_us': 'spacy_eng',
                    'eng_gb': 'spacy_eng',
                    'other': 'spacy_eng'
                }
            },
            'files': {
                'misc_settings': {
                    'read_files_in_chunks_lines': 1000
                }
            },
            'measures': {
                'lexical_density_diversity': {
                    'hdd': {'sample_size': 42},
                    'msttr': {'num_tokens_in_each_seg': 100},
                    'mtld': {'factor_size': 0.72},
                    'mattr': {'window_size': 100}
                }
            }
        }


class LanguageDetector:
    """æ™ºèƒ½è¯­è¨€æ£€æµ‹å™¨"""
    
    @staticmethod
    def detect_language(text):
        """
        è‡ªåŠ¨æ£€æµ‹æ–‡æœ¬è¯­è¨€
        è¿”å›: 'zho_cn', 'zho_tw', 'eng_us', 'eng_gb' æˆ– None
        """
        # ç»Ÿè®¡å­—ç¬¦ç±»å‹
        total_chars = len(text)
        if total_chars == 0:
            return None
        
        # ç»Ÿè®¡ä¸­æ–‡å­—ç¬¦ï¼ˆåŒ…æ‹¬æ ‡ç‚¹ï¼‰
        chinese_chars = len(re.findall(r'[\u4e00-\u9fff]', text))
        # ç»Ÿè®¡è‹±æ–‡å­—æ¯
        english_chars = len(re.findall(r'[a-zA-Z]', text))
        
        chinese_ratio = chinese_chars / total_chars
        english_ratio = english_chars / total_chars
        
        # åˆ¤æ–­é€»è¾‘
        if chinese_ratio > 0.3:  # ä¸­æ–‡å­—ç¬¦è¶…è¿‡30%
            # æ£€æµ‹æ˜¯ç®€ä½“è¿˜æ˜¯ç¹ä½“ï¼ˆç®€å•åˆ¤æ–­ï¼‰
            simplified_common = len(re.findall(r'[çš„æ˜¯åœ¨äº†æœ‰å’Œäººè¿™ä¸­å›½]', text))
            traditional_common = len(re.findall(r'[å€‹å€‘èªªè™•æ™‚é–“]', text))
            
            if traditional_common > simplified_common * 1.5:
                return 'zho_tw'
            else:
                return 'zho_cn'
                
        elif english_ratio > 0.5:  # è‹±æ–‡å­—æ¯è¶…è¿‡50%
            # ç®€å•åˆ¤æ–­è‹±å¼/ç¾å¼ï¼ˆåŸºäºå¸¸è§æ‹¼å†™å·®å¼‚ï¼‰
            british_patterns = len(re.findall(r'\b(colour|favour|honour|centre|theatre)\b', text.lower()))
            american_patterns = len(re.findall(r'\b(color|favor|honor|center|theater)\b', text.lower()))
            
            if british_patterns > american_patterns:
                return 'eng_gb'
            else:
                return 'eng_us'
        
        # é»˜è®¤è¿”å›è‹±æ–‡
        return 'eng_us'
    
    @staticmethod
    def get_language_name(lang_code):
        """è·å–è¯­è¨€åç§°"""
        names = {
            'zho_cn': 'ç®€ä½“ä¸­æ–‡',
            'zho_tw': 'ç¹ä½“ä¸­æ–‡',
            'eng_us': 'English (US)',
            'eng_gb': 'English (UK)'
        }
        return names.get(lang_code, 'æœªçŸ¥è¯­è¨€')


class DocumentAnalyzer:
    """æ™ºèƒ½æ–‡æ¡£åˆ†æå™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–åˆ†æå™¨"""
        self.main = MockMain()
        self.results = {}
        self.lang = None
        self.text = None
        
    def load_file(self, file_path):
        """åŠ è½½æ–‡ä»¶å¹¶è‡ªåŠ¨æ£€æµ‹è¯­è¨€"""
        try:
            # å°è¯•UTF-8ç¼–ç 
            with open(file_path, 'r', encoding='utf-8') as f:
                self.text = f.read().strip()
        except UnicodeDecodeError:
            # å°è¯•å…¶ä»–å¸¸è§ç¼–ç 
            for encoding in ['gbk', 'gb2312', 'big5', 'latin1']:
                try:
                    with open(file_path, 'r', encoding=encoding) as f:
                        self.text = f.read().strip()
                    print(f"âš ï¸  æ£€æµ‹åˆ°æ–‡ä»¶ç¼–ç ä¸º: {encoding}")
                    break
                except:
                    continue
            else:
                print(f"âŒ æ— æ³•è¯»å–æ–‡ä»¶ï¼Œè¯·ç¡®ä¿æ–‡ä»¶ç¼–ç æ­£ç¡®")
                return False
        except Exception as e:
            print(f"âŒ åŠ è½½æ–‡ä»¶å¤±è´¥: {e}")
            return False
        
        if not self.text:
            print(f"âŒ æ–‡ä»¶ä¸ºç©º")
            return False
        
        # è‡ªåŠ¨æ£€æµ‹è¯­è¨€
        self.lang = LanguageDetector.detect_language(self.text)
        if not self.lang:
            print(f"âŒ æ— æ³•è¯†åˆ«æ–‡æ¡£è¯­è¨€")
            return False
        
        return True
    
    def tokenize(self):
        """åˆ†è¯å¤„ç†"""
        try:
            # å¥å­åˆ†è¯
            self.sentences = wl_sentence_tokenization.wl_sentence_tokenize(
                self.main, self.text, lang=self.lang
            )
            
            # è¯è¯­åˆ†è¯
            self.tokens = []
            for sentence in self.sentences:
                tokens = wl_word_tokenization.wl_word_tokenize_flat(
                    self.main, sentence, lang=self.lang
                )
                self.tokens.extend(tokens)
            
            self.tokens_text = [str(token) for token in self.tokens]
            return True
        except Exception as e:
            print(f"âŒ åˆ†è¯å¤±è´¥: {e}")
            print(f"   å¯èƒ½éœ€è¦ä¸‹è½½è¯­è¨€æ¨¡å‹")
            if 'zho' in self.lang:
                print(f"   è¿è¡Œ: python3 -m spacy download zh_core_web_lg")
            else:
                print(f"   è¿è¡Œ: python3 -m spacy download en_core_web_lg")
            return False
    
    def calculate_metrics(self):
        """è®¡ç®—æ‰€æœ‰æŒ‡æ ‡"""
        num_sentences = len(self.sentences)
        num_words = len(self.tokens)
        num_chars = sum(len(token) for token in self.tokens_text)
        
        if num_sentences == 0 or num_words == 0:
            print("âš ï¸  æ–‡æœ¬è¿‡çŸ­ï¼Œæ— æ³•åˆ†æ")
            return False
        
        # 1. å¯è¯»æ€§æŒ‡æ ‡
        # ARI
        ari = 4.71 * (num_chars / num_words) + 0.5 * (num_words / num_sentences) - 21.43
        
        # Lix
        long_words = sum(1 for token in self.tokens_text if len(token) > 6)
        lix = (num_words / num_sentences) + (long_words * 100 / num_words)
        
        # Coleman-Liau
        L = (num_chars / num_words) * 100
        S = (num_sentences / num_words) * 100
        cli = 0.0588 * L - 0.296 * S - 15.8
        
        # 2. è¯æ±‡å¤šæ ·æ€§æŒ‡æ ‡
        num_types = len(set(self.tokens_text))
        ttr = num_types / num_words
        rttr = num_types / np.sqrt(num_words)
        cttr = num_types / np.sqrt(2 * num_words)
        
        if num_words > 1:
            herdan_c = np.log(num_types) / np.log(num_words)
        else:
            herdan_c = 0
        
        # Yule's K
        tokens_freq = Counter(self.tokens_text)
        freqs_count = Counter(tokens_freq.values())
        s2 = sum(freq ** 2 * count for freq, count in freqs_count.items())
        yule_k = 10000 * (s2 - num_words) / (num_words ** 2) if num_words > 0 else 0
        
        # 3. ç»“æ„å¤æ‚åº¦æŒ‡æ ‡
        sentence_lengths = []
        for sentence in self.sentences:
            tokens = wl_word_tokenization.wl_word_tokenize_flat(
                self.main, sentence, lang=self.lang
            )
            sentence_lengths.append(len(tokens))
        
        word_lengths = [len(token) for token in self.tokens_text]
        
        # 4. è¯é¢‘ç»Ÿè®¡ - è¿‡æ»¤æ ‡ç‚¹ç¬¦å·
        # åªç»Ÿè®¡åŒ…å«å­—æ¯æˆ–æ•°å­—çš„è¯
        tokens_freq_filtered = Counter([
            token for token in self.tokens_text 
            if any(c.isalnum() for c in token)  # è‡³å°‘åŒ…å«ä¸€ä¸ªå­—æ¯æˆ–æ•°å­—
        ])
        top_words = tokens_freq_filtered.most_common(20)
        
        # ä¿å­˜ç»“æœ
        self.results = {
            'language': {
                'detected': self.lang,
                'name': LanguageDetector.get_language_name(self.lang)
            },
            'readability': {
                'ARI': round(ari, 2),
                'Lix': round(lix, 2),
                'Coleman_Liau_Index': round(cli, 2),
                'interpretation': self._interpret_readability(ari, lix)
            },
            'lexical_diversity': {
                'num_tokens': num_words,
                'num_types': num_types,
                'TTR': round(ttr, 4),
                'RTTR': round(rttr, 4),
                'CTTR': round(cttr, 4),
                'Herdan_C': round(herdan_c, 4),
                'Yule_K': round(yule_k, 2),
                'interpretation': self._interpret_diversity(ttr)
            },
            'structural_complexity': {
                'num_sentences': num_sentences,
                'num_words': num_words,
                'num_chars': num_chars,
                'avg_sentence_length': round(np.mean(sentence_lengths), 2),
                'std_sentence_length': round(np.std(sentence_lengths), 2),
                'max_sentence_length': int(np.max(sentence_lengths)),
                'min_sentence_length': int(np.min(sentence_lengths)),
                'avg_word_length': round(np.mean(word_lengths), 2),
                'std_word_length': round(np.std(word_lengths), 2),
                'interpretation': self._interpret_complexity(np.mean(sentence_lengths))
            },
            'top_words': [(word, count) for word, count in top_words[:10]]
        }
        
        return True
    
    def _interpret_readability(self, ari, lix):
        """è§£é‡Šå¯è¯»æ€§"""
        if ari < 10 and lix < 40:
            return "æ˜“è¯» - é€‚åˆå¤§ä¼—è¯»è€…"
        elif ari < 14 and lix < 50:
            return "ä¸­ç­‰ - é€‚åˆé«˜ä¸­åŠä»¥ä¸Šè¯»è€…"
        else:
            return "å›°éš¾ - é€‚åˆä¸“ä¸šè¯»è€…"
    
    def _interpret_diversity(self, ttr):
        """è§£é‡Šè¯æ±‡å¤šæ ·æ€§"""
        if ttr > 0.6:
            return "ä¸°å¯Œ - è¯æ±‡ä½¿ç”¨å¤šæ ·"
        elif ttr > 0.5:
            return "ä¸­ç­‰ - è¯æ±‡ä½¿ç”¨é€‚ä¸­"
        else:
            return "é‡å¤ - è¯æ±‡é‡å¤è¾ƒå¤š"
    
    def _interpret_complexity(self, avg_sent_len):
        """è§£é‡Šç»“æ„å¤æ‚åº¦"""
        # æ ¹æ®è¯­è¨€è°ƒæ•´æ ‡å‡†
        if 'zho' in self.lang:
            if avg_sent_len < 20:
                return "ç®€å• - å¥å­ç»“æ„æ¸…æ™°"
            elif avg_sent_len < 30:
                return "ä¸­ç­‰ - å¥å­ç»“æ„é€‚ä¸­"
            else:
                return "å¤æ‚ - å¥å­è¾ƒé•¿"
        else:  # è‹±æ–‡
            if avg_sent_len < 15:
                return "ç®€å• - å¥å­ç»“æ„æ¸…æ™°"
            elif avg_sent_len < 20:
                return "ä¸­ç­‰ - å¥å­ç»“æ„é€‚ä¸­"
            else:
                return "å¤æ‚ - å¥å­è¾ƒé•¿"
    
    def print_results(self):
        """æ‰“å°åˆ†æç»“æœ"""
        print(f"\n{'='*60}")
        print(f"  æ™ºèƒ½æ–‡æ¡£åˆ†ææŠ¥å‘Š")
        print(f"{'='*60}\n")
        
        # è¯­è¨€ä¿¡æ¯
        print(f"ğŸ“Œ æ£€æµ‹è¯­è¨€: {self.results['language']['name']}")
        print(f"   æ–‡æ¡£è§„æ¨¡: {self.results['structural_complexity']['num_words']} è¯, "
              f"{self.results['structural_complexity']['num_sentences']} å¥\n")
        
        # å¯è¯»æ€§
        print(f"ğŸ“– å¯è¯»æ€§åˆ†æ:")
        print(f"   ARIæŒ‡æ•°: {self.results['readability']['ARI']}")
        print(f"   LixæŒ‡æ•°: {self.results['readability']['Lix']}")
        print(f"   Coleman-Liau: {self.results['readability']['Coleman_Liau_Index']}")
        print(f"   ğŸ’¡ {self.results['readability']['interpretation']}\n")
        
        # è¯æ±‡å¤šæ ·æ€§
        print(f"ğŸ“š è¯æ±‡å¤šæ ·æ€§:")
        print(f"   è¯å‹æ•°/è¯ç¬¦æ•°: {self.results['lexical_diversity']['num_types']}/"
              f"{self.results['lexical_diversity']['num_tokens']}")
        print(f"   TTR: {self.results['lexical_diversity']['TTR']}")
        print(f"   RTTR: {self.results['lexical_diversity']['RTTR']}")
        print(f"   Herdan's C: {self.results['lexical_diversity']['Herdan_C']}")
        print(f"   ğŸ’¡ {self.results['lexical_diversity']['interpretation']}\n")
        
        # ç»“æ„å¤æ‚åº¦
        print(f"ğŸ” ç»“æ„å¤æ‚åº¦:")
        print(f"   å¹³å‡å¥é•¿: {self.results['structural_complexity']['avg_sentence_length']} è¯")
        print(f"   å¥é•¿æ ‡å‡†å·®: {self.results['structural_complexity']['std_sentence_length']}")
        print(f"   å¹³å‡è¯é•¿: {self.results['structural_complexity']['avg_word_length']} å­—ç¬¦")
        print(f"   ğŸ’¡ {self.results['structural_complexity']['interpretation']}\n")
        
        # é«˜é¢‘è¯
        print(f"ğŸ“ˆ é«˜é¢‘è¯ Top 10:")
        for i, (word, count) in enumerate(self.results['top_words'], 1):
            print(f"   {i:2d}. {word:<15} ({count} æ¬¡)")
        
        print(f"\n{'='*60}\n")
    
    def save_results(self, output_path):
        """ä¿å­˜ç»“æœä¸ºJSON"""
        try:
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(self.results, f, ensure_ascii=False, indent=2)
            print(f"âœ… ç»“æœå·²ä¿å­˜è‡³: {output_path}")
        except Exception as e:
            print(f"âŒ ä¿å­˜å¤±è´¥: {e}")
    
    def analyze(self, file_path, output_json=None):
        """æ‰§è¡Œå®Œæ•´åˆ†æ"""
        print(f"\nğŸš€ å¼€å§‹åˆ†ææ–‡æ¡£: {file_path}\n")
        
        # åŠ è½½æ–‡ä»¶
        if not self.load_file(file_path):
            return False
        
        print(f"âœ… æ–‡ä»¶åŠ è½½æˆåŠŸ ({len(self.text)} å­—ç¬¦)")
        print(f"âœ… è¯­è¨€è¯†åˆ«: {LanguageDetector.get_language_name(self.lang)}\n")
        
        # åˆ†è¯
        print(f"ğŸ“ æ­£åœ¨åˆ†è¯...")
        if not self.tokenize():
            return False
        
        print(f"âœ… åˆ†è¯å®Œæˆ: {len(self.sentences)} ä¸ªå¥å­, {len(self.tokens)} ä¸ªè¯\n")
        
        # è®¡ç®—æŒ‡æ ‡
        print(f"ğŸ“Š æ­£åœ¨è®¡ç®—æŒ‡æ ‡...")
        if not self.calculate_metrics():
            return False
        
        print(f"âœ… è®¡ç®—å®Œæˆ")
        
        # æ˜¾ç¤ºç»“æœ
        self.print_results()
        
        # ä¿å­˜JSON
        if output_json:
            self.save_results(output_json)
        
        return True


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description='æ™ºèƒ½æ–‡æ¡£åˆ†æå™¨ - è‡ªåŠ¨è¯†åˆ«ä¸­è‹±æ–‡',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  åŸºç¡€ä½¿ç”¨ï¼ˆè‡ªåŠ¨è¯†åˆ«è¯­è¨€ï¼‰:
    python analyze_doc.py document.txt
    
  ä¿å­˜JSONç»“æœ:
    python analyze_doc.py document.txt --output results.json
    python analyze_doc.py document.txt -o results.json

æ”¯æŒçš„è¯­è¨€:
  - ç®€ä½“ä¸­æ–‡ (è‡ªåŠ¨è¯†åˆ«)
  - ç¹ä½“ä¸­æ–‡ (è‡ªåŠ¨è¯†åˆ«)
  - English US (è‡ªåŠ¨è¯†åˆ«)
  - English UK (è‡ªåŠ¨è¯†åˆ«)

åˆ†ææŒ‡æ ‡:
  ğŸ“– å¯è¯»æ€§: ARI, Lix, Coleman-LiauæŒ‡æ•°
  ğŸ“š è¯æ±‡å¤šæ ·æ€§: TTR, RTTR, CTTR, Herdan's C, Yule's K
  ğŸ” ç»“æ„å¤æ‚åº¦: å¥é•¿ã€è¯é•¿ç»Ÿè®¡
  ğŸ“ˆ è¯é¢‘åˆ†æ: Top 10 é«˜é¢‘è¯
        """
    )
    
    parser.add_argument(
        'file',
        help='è¦åˆ†æçš„æ–‡æ¡£æ–‡ä»¶è·¯å¾„'
    )
    
    parser.add_argument(
        '--output', '-o',
        help='è¾“å‡ºJSONç»“æœæ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰'
    )
    
    args = parser.parse_args()
    
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(args.file):
        print(f"âŒ é”™è¯¯: æ–‡ä»¶ä¸å­˜åœ¨: {args.file}")
        return 1
    
    # åˆ›å»ºåˆ†æå™¨å¹¶æ‰§è¡Œåˆ†æ
    analyzer = DocumentAnalyzer()
    success = analyzer.analyze(args.file, output_json=args.output)
    
    return 0 if success else 1


if __name__ == '__main__':
    sys.exit(main())

